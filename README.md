# ğŸ” Frequency Regularization: A Spectral Analysis of Deep Neural Networks
# ğŸ” é¢‘ç‡æ­£åˆ™åŒ–ï¼šæ·±åº¦ç¥ç»ç½‘ç»œçš„å…‰è°±åˆ†æ

> **Unveiling the "Invisible" Low-Pass Filtering Effect of Regularization in Modern CNNs.**  
> **æ­ç¤ºæ­£åˆ™åŒ–åœ¨ç°ä»£ CNN ä¸­çš„"éšå½¢"ä½é€šæ»¤æ³¢æ•ˆåº”ã€‚**

[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-blue.svg?style=for-the-badge)](LICENSE)

## ğŸ“– Introduction | ç®€ä»‹

Why does L2 Regularization (Weight Decay) improve generalization? The common answer is "it keeps weights small." But **what does that mean physically?**

This project investigates the **Spectral Bias** of Neural Networks. By analyzing ResNet-18 on CIFAR-10 in the frequency domain, we provide visual and quantitative evidence that **Regularization acts as a Low-Pass Filter**, forcing the model to learn robust low-frequency shapes while ignoring high-frequency noise.

**ï¼š**  
ä¸ºä»€ä¹ˆ L2 æ­£åˆ™åŒ–ï¼ˆæƒé‡è¡°å‡ï¼‰èƒ½æå‡æ³›åŒ–èƒ½åŠ›ï¼Ÿå¸¸è§çš„ç­”æ¡ˆæ˜¯"å®ƒè®©æƒé‡å˜å°"ã€‚ä½†**è¿™åœ¨ç‰©ç†ä¸Šæ„å‘³ç€ä»€ä¹ˆï¼Ÿ**

æœ¬é¡¹ç›®ç ”ç©¶ç¥ç»ç½‘ç»œçš„**é¢‘è°±åå·®ï¼ˆSpectral Biasï¼‰**ã€‚é€šè¿‡åœ¨é¢‘åŸŸåˆ†æ ResNet-18 åœ¨ CIFAR-10 ä¸Šçš„è¡¨ç°ï¼Œæˆ‘ä»¬æä¾›äº†è§†è§‰å’Œå®šé‡è¯æ®ï¼Œè¯æ˜**æ­£åˆ™åŒ–èµ·åˆ°äº†ä½é€šæ»¤æ³¢å™¨çš„ä½œç”¨**ï¼Œè¿«ä½¿æ¨¡å‹å­¦ä¹ é²æ£’çš„ä½é¢‘å½¢çŠ¶ï¼ŒåŒæ—¶å¿½ç•¥é«˜é¢‘å™ªå£°ã€‚

## ğŸš€ Key Contributions | æ ¸å¿ƒè´¡çŒ®


1.  **Visual Diagnostic Framework:** A dynamic heatmap system that tracks how weight frequencies evolve during training.
2.  **SSR Metric (Spectral Suppression Ratio):** A novel metric proposed to quantify the intensity of high-frequency suppression.
3.  **Engineering Optimization:** Implemented a **Discrete Radial Profiling** algorithm to enable accurate FFT analysis on small ($3\times3$) convolutional kernels.
4.  **Robustness Discovery:** Revealed that L2 models are highly robust to **Low Resolution** (High-Freq Loss) but sensitive to **Gaussian Noise**, proving their reliance on low-frequency structures.


1.  **å¯è§†åŒ–è¯Šæ–­æ¡†æ¶ï¼š** åŠ¨æ€çƒ­å›¾ç³»ç»Ÿï¼Œè¿½è¸ªæƒé‡é¢‘ç‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¼”å˜ã€‚
2.  **SSR æŒ‡æ ‡ï¼ˆé¢‘è°±æŠ‘åˆ¶æ¯”ï¼‰ï¼š** æå‡ºçš„æ–°æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–é«˜é¢‘æŠ‘åˆ¶çš„å¼ºåº¦ã€‚
3.  **å·¥ç¨‹ä¼˜åŒ–ï¼š** å®ç°äº†**ç¦»æ•£å¾„å‘è½®å»“åˆ†æ**ç®—æ³•ï¼Œä½¿å°å°ºå¯¸ï¼ˆ$3\times3$ï¼‰å·ç§¯æ ¸çš„ FFT åˆ†ææˆä¸ºå¯èƒ½ã€‚
4.  **é²æ£’æ€§å‘ç°ï¼š** æ­ç¤ºäº† L2 æ¨¡å‹å¯¹**ä½åˆ†è¾¨ç‡**ï¼ˆé«˜é¢‘æŸå¤±ï¼‰é«˜åº¦é²æ£’ï¼Œä½†å¯¹**é«˜æ–¯å™ªå£°**æ•æ„Ÿï¼Œè¯æ˜äº†å…¶å¯¹ä½é¢‘ç»“æ„çš„ä¾èµ–ã€‚

---

## ğŸ“Š Main Results | ä¸»è¦ç»“æœ

### 1. The "Invisible" Filter (Spectral Evolution) | "éšå½¢"æ»¤æ³¢å™¨ï¼ˆé¢‘è°±æ¼”å˜ï¼‰

**:**  
*Left: No Regularization (High-freq noise accumulates). Right: L2 Regularization (High-freq stays dark/suppressed).*

**ï¼š**  
*å·¦å›¾ï¼šæ— æ­£åˆ™åŒ–ï¼ˆé«˜é¢‘å™ªå£°ç´¯ç§¯ï¼‰ã€‚å³å›¾ï¼šL2 æ­£åˆ™åŒ–ï¼ˆé«˜é¢‘ä¿æŒæš—è‰²/è¢«æŠ‘åˆ¶ï¼‰ã€‚*

![Spectrum Evolution](results/figures/exp_b_spectrum_evolution.png)

**:** *(Note: The top rows represent high frequencies. L2 keeps them "clean".)*  
**ï¼š** *ï¼ˆæ³¨ï¼šé¡¶éƒ¨è¡Œä»£è¡¨é«˜é¢‘ã€‚L2 ä¿æŒå®ƒä»¬"å¹²å‡€"ã€‚ï¼‰*

### 2. Quantifying the Suppression (SSR) | é‡åŒ–æŠ‘åˆ¶ï¼ˆSSRï¼‰

**:**  
Using our **SSR Metric**, we found that L2 regularization suppresses the growth of high-frequency energy by **~3x** compared to the baseline.

**ï¼š**  
ä½¿ç”¨æˆ‘ä»¬çš„**SSR æŒ‡æ ‡**ï¼Œæˆ‘ä»¬å‘ç° L2 æ­£åˆ™åŒ–å°†é«˜é¢‘èƒ½é‡çš„å¢é•¿æŠ‘åˆ¶äº†**çº¦ 3 å€**ï¼Œç›¸æ¯”åŸºçº¿æ¨¡å‹ã€‚

![SSR Comparison](results/figures/exp_b_ssr_comparison.png)

### 3. The Robustness Proof (Low Resolution) | é²æ£’æ€§è¯æ˜ï¼ˆä½åˆ†è¾¨ç‡ï¼‰

**:**  
When image resolution drops (simulating high-frequency loss), the L2 model (Green) significantly outperforms the Baseline (Blue). **At 24x24 px, L2 leads by >6%.**

**ï¼š**  
å½“å›¾åƒåˆ†è¾¨ç‡é™ä½ï¼ˆæ¨¡æ‹Ÿé«˜é¢‘æŸå¤±ï¼‰æ—¶ï¼ŒL2 æ¨¡å‹ï¼ˆç»¿è‰²ï¼‰æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼ˆè“è‰²ï¼‰ã€‚**åœ¨ 24x24 åƒç´ ä¸‹ï¼ŒL2 é¢†å…ˆ >6%ã€‚**

![Resolution Robustness](results/figures/exp_d_resolution_robustness.png)

---

## ğŸ› ï¸ Installation & Usage | å®‰è£…ä¸ä½¿ç”¨

### 1. Clone the repository | å…‹éš†ä»“åº“
```bash
git clone https://github.com/yourusername/FrequencyRegularization.git
cd FrequencyRegularization
```

### 2. Install dependencies | å®‰è£…ä¾èµ–
```bash
pip install torch torchvision numpy matplotlib pandas tqdm scipy
```

### 3. Run Experiments | è¿è¡Œå®éªŒ

**Experiment A: Synthetic Demo (Proof of Concept) | å®éªŒ Aï¼šåˆæˆæ•°æ®æ¼”ç¤ºï¼ˆæ¦‚å¿µéªŒè¯ï¼‰**
```bash
python experiments/exp_a_synthetic.py
```

**Experiment B: Spectral Evolution & SSR (Core Analysis) | å®éªŒ Bï¼šé¢‘è°±æ¼”å˜ä¸ SSRï¼ˆæ ¸å¿ƒåˆ†æï¼‰**  
**:** Trains ResNet-18 models and generates spectral heatmaps.  
**ï¼š** è®­ç»ƒ ResNet-18 æ¨¡å‹å¹¶ç”Ÿæˆé¢‘è°±çƒ­å›¾ã€‚
```bash
python experiments/exp_b_spectrum_evolution.py
```

**Experiment C: Noise Robustness (The Trade-off) | å®éªŒ Cï¼šå™ªå£°é²æ£’æ€§ï¼ˆæƒè¡¡ï¼‰**  
**:** Tests models against Gaussian Noise.  
**ï¼š** æµ‹è¯•æ¨¡å‹å¯¹é«˜æ–¯å™ªå£°çš„é²æ£’æ€§ã€‚
```bash
python experiments/exp_c_robustness.py
```

**Experiment D: Resolution Robustness (The Advantage) | å®éªŒ Dï¼šåˆ†è¾¨ç‡é²æ£’æ€§ï¼ˆä¼˜åŠ¿ï¼‰**  
**:** Tests models against Low Resolution / Blur.  
**ï¼š** æµ‹è¯•æ¨¡å‹å¯¹ä½åˆ†è¾¨ç‡/æ¨¡ç³Šçš„é²æ£’æ€§ã€‚
```bash
python experiments/exp_d_resolution_robustness.py
```

## ğŸ“‚ Project Structure | é¡¹ç›®ç»“æ„

```
FrequencyRegularization/
â”œâ”€â”€ data/               # Data loaders and preprocessing | æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
â”œâ”€â”€ models/             # ResNet implementation | ResNet å®ç°
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ frequency.py    # [CORE] FFT & Radial Profile algorithms (Optimized for 3x3 kernels) | [æ ¸å¿ƒ] FFT ä¸å¾„å‘è½®å»“ç®—æ³•ï¼ˆé’ˆå¯¹ 3x3 æ ¸ä¼˜åŒ–ï¼‰
â”‚   â””â”€â”€ visualizer.py   # Plotting tools | ç»˜å›¾å·¥å…·
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ exp_b_spectrum_evolution.py  # Training & Frequency Analysis | è®­ç»ƒä¸é¢‘ç‡åˆ†æ
â”‚   â””â”€â”€ exp_d_resolution_robustness.py # Robustness Testing | é²æ£’æ€§æµ‹è¯•
â”œâ”€â”€ results/            # Output figures and checkpoints | è¾“å‡ºå›¾è¡¨ä¸æ£€æŸ¥ç‚¹
â””â”€â”€ README.md
```

## ğŸ§  Theory Reference | ç†è®ºå‚è€ƒ

**:**  
This project builds upon the theory of Spectral Bias:
- Rahaman et al., "On the Spectral Bias of Neural Networks", ICML 2019.
- Xu et al., "Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks", 2019.

**ï¼š**  
æœ¬é¡¹ç›®åŸºäºé¢‘è°±åå·®ç†è®ºï¼š
- Rahaman ç­‰äººï¼Œ"On the Spectral Bias of Neural Networks", ICML 2019.
- Xu ç­‰äººï¼Œ"Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks", 2019.
